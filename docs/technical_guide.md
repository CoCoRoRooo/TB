# üìñ Documentation Technique du Projet

## üìå Introduction

Ce projet est une application Flask qui permet aux utilisateurs de poser des questions et d'obtenir des r√©ponses bas√©es sur un ensemble de guides techniques extraits de l'API iFixit. L'application utilise le traitement du langage naturel et la recherche vectorielle pour proposer des r√©ponses pertinentes.

---

## üìÇ Structure des fichiers

- **`data/guides.json`** : Contient les guides techniques extraits de l'API iFixit.
- **`data/guides.py`** : Script permettant de r√©cup√©rer les guides depuis l'API iFixit et de les stocker localement.
- **`tokenizer.py`** : Permet de convertir les guides en vecteurs et d'effectuer des recherches efficaces.
- **`llm.py`** : Interagit avec l'API OpenAI/Ollama pour g√©n√©rer des r√©ponses aux questions des utilisateurs.
- **`app.py`** : Contient l'application Flask qui expose les endpoints pour l'interaction utilisateur.
- **`templates/index.html`** : Interface utilisateur pour poser des questions.

---

## üìù D√©tails des fichiers

### üìÅ `data/guides.json`

#### üìÑ Contenu

Fichier JSON contenant une liste de guides techniques. Chaque guide est repr√©sent√© sous la forme d'un objet avec les champs suivants :

- **`guideid`** : Identifiant unique du guide.
- **`locale`** : Langue du guide.
- **`title`** : Titre du guide.
- **`summary`** : Br√®ve description.
- **`difficulty`** : Niveau de difficult√©.
- **`url`** : Lien vers le guide sur iFixit.
- **`image`** : Liens vers diff√©rentes versions d'images associ√©es.

---

### üìÅ `data/guides.py`

#### üîπ Fonction `fetch_all_guides()`

Cette fonction permet de r√©cup√©rer tous les guides disponibles via l'API iFixit. Elle g√®re la pagination avec les param√®tres `offset` et `limit`, et traite les erreurs JSON et les limitations de requ√™tes API.

- Utilise `requests.get` pour interroger l'API d'iFixit.
- Si la requ√™te est r√©ussie, elle √©tend la liste des guides r√©cup√©r√©s avec `all_guides`.
- En cas de limitation de requ√™tes `(status code 429)`, elle met en pause - l'ex√©cution pour 30 secondes avant de r√©essayer.

```python
def fetch_all_guides():
    all_guides = []
    offset = 0
    limit = 20
    has_more = True

    while has_more:
        url = f"https://www.ifixit.com/api/2.0/guides?offset={offset}&limit={limit}"
        response = requests.get(url)

        # V√©rifiez la r√©ponse
        print(f"Status Code: {response.status_code}")
        print(
            f"Response Content: {response.content[:200]}..."
        )  # Affiche les premiers 200 caract√®res de la r√©ponse

        if response.status_code == 200:
            try:
                guides = response.json()
                all_guides.extend(guides)

                if len(guides) < limit:
                    has_more = False
                else:
                    offset += limit
            except ValueError:
                print("Erreur de d√©codage JSON")
                break
        elif response.status_code == 429:
            # Si la limite est atteinte, attendons 30 secondes avant de continuer
            print("Limite de requ√™tes atteinte. Attente de 30 secondes...")
            time.sleep(30)  # Attendre 30 secondes avant de r√©essayer
        else:
            print(
                f"Erreur lors de la r√©cup√©ration des donn√©es. Code statut: {response.status_code}"
            )
            break

    return all_guides
```

---

### üìÅ `tokenizer.py`

#### üîπ Fonction `load_guides(file_path)`

Charge les guides depuis le fichier JSON pour les transformer en vecteurs. Ce fichier est utilis√© pour initialiser les donn√©es en amont du traitement par LangChain.

#### üîπ Fonction `index_guides(guides, model_name="all-MiniLM-L6-v2")`

Cette fonction est responsable de la cr√©ation des embeddings pour chaque guide. Elle utilise LangChain pour cr√©er des objets `Document` et les indexer dans une base FAISS (Fast Approximate Nearest Neighbor Search). FAISS permet d'effectuer des recherches rapides dans les vecteurs d'embeddings.

- Les guides sont convertis en objets `Document` compatibles avec LangChain.
- Utilisation du mod√®le `SentenceTransformer` (`ici all-MiniLM-L6-v2`) pour g√©n√©rer des embeddings des guides.
- La base FAISS permet ensuite une recherche rapide et efficace dans ces embeddings.
- La fonction retourne un `Retriever` LangChain, utilis√© pour interroger les documents index√©s.

```python
# Convertir les guides en vecteurs et cr√©er un retriever LangChain
def index_guides(guides, model_name="sentence-transformers/paraphrase-MiniLM-L6-v2"):
    # Construire les textes et les objets Document
    documents = [
        Document(
            page_content=f"{g['dataType']} - {g['type']} {g['subject']} : {g['title']} {(g['url'])}",
            metadata={
                "url": g["url"],
                "type": g["type"],
                "subject": g["subject"],
                "title": g["title"],
            },
        )
        for g in guides
    ]

    # Cr√©er des embeddings avec LangChain
    embedding_model = HuggingFaceEmbeddings(model_name=model_name)
    vector_store = FAISS.from_documents(documents, embedding_model)

    return vector_store.as_retriever(), embedding_model
```

### üìÅ `llm.py`

#### üîπ Classe `OllamaLLM`

Cette classe permet d'utiliser un mod√®le local Ollama pour g√©n√©rer des r√©ponses en fonction des prompts. Elle repose sur la librairie `ollama` pour communiquer avec un mod√®le de traitement de langage.

```python
class OllamaLLM(LLM):
    model: str = "mistral"

    def _call(self, prompt: str, stop: List[str] = None) -> str:
        response = ollama.chat(
            model=self.model, messages=[{"role": "user", "content": prompt}]
        )
        return response["message"]["content"]

    @property
    def _identifying_params(self) -> dict:
        return {"model": self.model}

    @property
    def _llm_type(self) -> str:
        return "ollama"
```

#### üîπ Fonction `create_rag_chain(retriever)`

Cette fonction cr√©e une cha√Æne RAG (Retrieval-Augmented Generation). Elle r√©cup√®re des documents pertinents via le `retriever` LangChain et g√©n√®re une r√©ponse en utilisant un mod√®le LLM, comme Ollama ou OpenAI.

```python
# Cha√Æne RAG
def create_rag_chain(retriever):
    return (
        {
            "context": retriever | format_documents,
            "question": RunnablePassthrough(),
        }
        | prompt_template
        | llm
        | StrOutputParser()
    )
```

##### Fonctionnement de `create_rag_chain(retriever)`

La fonction est utilis√©e pour configurer une cha√Æne de traitement qui :

- R√©cup√®re des documents pertinents via un m√©canisme de r√©cup√©ration d'information (le `retriever` LangChain).
- Formate ces documents pour les rendre compatibles avec un mod√®le de g√©n√©ration de texte (par exemple, Ollama ou OpenAI).
- Applique un prompt template pour construire un contexte clair et structur√© pour le mod√®le de g√©n√©ration de texte.
- G√©n√®re une r√©ponse en utilisant un mod√®le de langage (LLM, par exemple Ollama ou OpenAI).
- Parse la sortie pour en faire une r√©ponse structur√©e.

##### √âtape par √©tape

1. `"context": retriever | format_documents`

   - But : Cette partie de la cha√Æne est responsable de la r√©cup√©ration des documents pertinents pour la question de l'utilisateur.
   - Explication d√©taill√©e :
     - `retriever` est un objet LangChain, g√©n√©ralement un `Retriever` bas√© sur un moteur de recherche vectorielle comme FAISS. Il est utilis√© pour r√©cup√©rer les documents les plus pertinents par rapport √† une question pos√©e.
     - Ensuite, `| format_documents` est une op√©ration qui prend les documents r√©cup√©r√©s et les transforme en un format lisible par le mod√®le de langage. `format_documents` cr√©e une cha√Æne de texte en concat√©nant les contenus des documents avec des s√©parateurs appropri√©s. Cette √©tape permet au mod√®le LLM de comprendre facilement les informations extraites.
   - Importance : Sans cette √©tape, le mod√®le de g√©n√©ration de texte ne disposerait d'aucun contexte pertinent sur lequel se baser pour g√©n√©rer une r√©ponse.

2. `"question": RunnablePassthrough()`

   - But : Cette partie passe la question de l'utilisateur telle quelle √† l'√©tape suivante sans la modifier.
   - Explication d√©taill√©e :
     - `RunnablePassthrough` est un composant de LangChain qui prend un objet ou une valeur en entr√©e et la transmet telle quelle en sortie. Ici, il est utilis√© pour s'assurer que la question de l'utilisateur est transmise sans modification √† l'√©tape suivante.
   - Importance : Il est crucial que la question de l'utilisateur soit intacte pour qu'elle puisse √™tre int√©gr√©e dans le prompt template pour la g√©n√©ration de texte.

3. `| prompt_template`

   - But : Appliquer un mod√®le de prompt sur le contexte et la question pour formater correctement la demande avant de la transmettre au mod√®le LLM.
   - Explication d√©taill√©e :

     - `prompt_template` est un objet LangChain de type `PromptTemplate` qui d√©finit la structure du prompt. Il utilise deux variables : `context` (les documents r√©cup√©r√©s) et `question` (la question pos√©e par l'utilisateur). Le prompt template construit un message structur√© qui sera envoy√© au mod√®le LLM pour g√©n√©rer une r√©ponse.
     - Exemple de prompt :

       ```python
        L'utilisateur pose la question : {question}

        Voici des guides pertinents :
        {context}

        R√©ponds en t'appuyant sur ces guides.
       ```

   - Importance : Cette √©tape est essentielle pour fournir au mod√®le LLM un format de question et de contexte structur√© et facile √† traiter, ce qui augmente la pertinence et la qualit√© de la r√©ponse g√©n√©r√©e.

4. `| llm`

   - But : Cette √©tape utilise un mod√®le de langage (LLM) pour g√©n√©rer une r√©ponse √† partir du prompt format√©.
   - Explication d√©taill√©e :
     - Le mod√®le LLM (par exemple, Ollama ou OpenAI) prend le prompt format√© et g√©n√®re une r√©ponse bas√©e sur son entra√Ænement et les informations fournies dans le contexte.
   - Importance : Le LLM est le c≈ìur du processus de g√©n√©ration de la r√©ponse. Sans lui, il n'y aurait aucune g√©n√©ration automatique de texte. C'est lui qui utilise les informations extraites des guides pour fournir une r√©ponse coh√©rente et pertinente √† l'utilisateur.

5. `| StrOutputParser()`

   - But : Cette √©tape sert √† parser et √† formater la sortie g√©n√©r√©e par le mod√®le LLM, afin de la rendre propre et structur√©e.
   - Explication d√©taill√©e :
     - `StrOutputParser()` est un composant de LangChain qui prend la sortie du mod√®le de langage et la transforme en une cha√Æne de caract√®res lisible et propre.
   - Importance : Cette √©tape permet de nettoyer la sortie g√©n√©r√©e par le mod√®le pour s'assurer que la r√©ponse est claire et bien format√©e avant de l'envoyer √† l'utilisateur.

##### Ordre d'appel et syntaxe

- Ordre d'ex√©cution : La syntaxe de LangChain utilise le "pipe" (`|`) pour encha√Æner les √©tapes, ce qui signifie que chaque composant de la cha√Æne re√ßoit la sortie du composant pr√©c√©dent comme entr√©e.
- Flux d'ex√©cution :
  1. Les documents sont r√©cup√©r√©s et format√©s.
  2. La question de l'utilisateur est transmise sans modification.
  3. Un prompt structur√© est cr√©√© avec le contexte et la question.
  4. Le mod√®le LLM g√©n√®re une r√©ponse en fonction du prompt.
  5. La r√©ponse g√©n√©r√©e est ensuite nettoy√©e et renvoy√©e sous forme de texte lisible.

##### Importance de chaque √©tape

1. R√©cup√©ration des documents (`retriever`) : Sans la recherche d'informations pertinentes, le mod√®le ne pourrait pas r√©pondre de mani√®re appropri√©e. Cette √©tape garantit que le mod√®le re√ßoit des donn√©es fiables et cibl√©es.

2. Formatage des documents (`format_documents`) : La conversion des documents en un format compr√©hensible pour le mod√®le est cruciale pour assurer que le LLM peut extraire les informations pertinentes et g√©n√©rer une r√©ponse coh√©rente.

3. Construction du prompt : Un bon prompt est essentiel pour guider le mod√®le vers une r√©ponse correcte et pr√©cise. Un prompt mal con√ßu peut entra√Æner des r√©ponses vagues ou inexactes.

4. Mod√®le LLM : Le mod√®le LLM transforme le prompt structur√© en une r√©ponse textuelle. Sa qualit√© et sa capacit√© √† comprendre le contexte sont cruciales pour obtenir une r√©ponse utile.

5. Parsing de la sortie : Une sortie propre et bien format√©e est essentielle pour garantir que l'utilisateur re√ßoit une r√©ponse lisible et claire.

#### üîπ D√©finition du prompt

Le `PromptTemplate` d√©finit comment la question et les documents pertinents doivent √™tre pr√©sent√©s au mod√®le LLM pour g√©n√©rer une r√©ponse.

```python
# D√©finir le prompt
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="L'utilisateur pose la question : {question}\n\nVoici des guides pertinents :\n{context}\n\nR√©ponds en t'appuyant sur ces guides.",
)
```

#### üîπ Fonction `format_documents(docs)`

Cette fonction extrait le contenu des documents trouv√©s par le retriever et le pr√©pare pour l'input au mod√®le.

```python
# Fonction qui r√©cup√®re le contenu des documents trouv√©s
def format_documents(docs):
    return "\n\n".join(doc.page_content for doc in docs)
```

---

### üìÅ `app.py`

#### üîπ Charger et indexer les guides au d√©marrage du serveur

Au d√©marrage du serveur Flask, cette ligne charge et indexe les guides depuis `guides.json`, en utilisant les fonctions d√©finies dans `tokenizer.py`.

```python
guides = load_guides("data/guides.json")
retriever, embed_model = index_guides(guides)
```

#### üîπ Construire la cha√Æne RAG

Cette ligne cr√©e la cha√Æne RAG avec le retriever LangChain.

```python
rag_chain = create_rag_chain(retriever)
```

#### üîπ Endpoint `/`

Affiche la page d'accueil avec un champ pour poser des questions.

```python
def home():
    return render_template("index.html")
```

#### üîπ Endpoint `/ask`

- R√©cup√®re la requ√™te utilisateur.
- Recherche les guides pertinents via le retriever LangChain.
- G√©n√®re une r√©ponse avec la cha√Æne RAG.

```python
@app.route("/ask", methods=["POST"])
def ask():
    data = request.json
    user_query = data.get("query", "")
    if not user_query:
        return jsonify({"error": "Aucune question fournie"}), 400
    response = rag_chain.invoke(user_query)
    return jsonify({"query": user_query, "response": response})
```

---

### üìÅ `templates/index.html`

Interface utilisateur simple permettant de poser des questions.

- Champ de saisie utilisateur.
- Bouton d'envoi.
- Affichage de la r√©ponse re√ßue.

```html
<input
  type="text"
  id="questionInput"
  class="form-control"
  placeholder="Ex: Mon iPhone ne charge plus"
/>
<button class="btn btn-primary w-100" onclick="askQuestion()">
  Poser la question
</button>
```

---

## üöÄ Conclusion

Ce projet permet d'exploiter des guides techniques via une recherche optimis√©e et une intelligence artificielle g√©n√©rative. L'int√©gration de FAISS pour la recherche vectorielle et de GPT-4 pour la g√©n√©ration de r√©ponses offre un assistant technique performant et efficace.
