from langchain_openai import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.language_models import LLM
from typing import Any, List
import ollama
import os
import requests
from dotenv import load_dotenv


class OllamaLLM(LLM):
    model: str = "llama3"  # mistral, llama3

    def _call(self, prompt: str, stop: List[str] = None) -> str:
        response = ollama.chat(
            model=self.model, messages=[{"role": "user", "content": prompt}]
        )
        return response["message"]["content"]

    @property
    def _identifying_params(self) -> dict:
        return {"model": self.model}

    @property
    def _llm_type(self) -> str:
        return "ollama"


# Charger les variables d'environnement
load_dotenv()
OPENAI_KEY = os.getenv("OPENAI_KEY")

# Utiliser un modÃ¨le OpenAI ou Ollama
use_ollama = False  # Mettre Ã  False pour utiliser OpenAI

if not use_ollama:
    llm = ChatOpenAI(openai_api_key=OPENAI_KEY, model="gpt-4", temperature=0.5)
else:
    llm = OllamaLLM()

prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
L'utilisateur pose la question suivante :

â¡ï¸ {question}

Tu disposes ci-dessous d'une sÃ©lection de guides techniques pertinents. Ces guides incluent des descriptions gÃ©nÃ©rales ainsi que des **Ã©tapes de rÃ©solution (Steps)** dÃ©taillÃ©es.

ğŸ¯ Ta mission :
- Exploite au maximum les **Ã©tapes (Steps)** fournies dans les guides pour construire ta rÃ©ponse.
- Fournis une rÃ©ponse claire, structurÃ©e et complÃ¨te en franÃ§ais.
- Formule des instructions pratiques en t'appuyant prÃ©cisÃ©ment sur les Ã©tapes, tout en adaptant ton discours au contexte du problÃ¨me posÃ©.
- Donne la source du guide.

ğŸ“š Guides disponibles :

{context}

ğŸ›  Format de rÃ©ponse attendu :
---
ğŸ” **Analyse du problÃ¨me** :
[PrÃ©sente une synthÃ¨se du problÃ¨me posÃ© et ce que les guides indiquent Ã  son sujet.]

âœ… **VÃ©rifications prÃ©alables recommandÃ©es** :
[Liste les Ã©lÃ©ments Ã  inspecter ou tester avant de dÃ©marrer la procÃ©dure.]

ğŸ“ **ProcÃ©dure dÃ©taillÃ©e (basÃ©e sur les Steps)** :
Step 1 : [DÃ©cris l'Ã©tape avec clartÃ©, en reformulant si nÃ©cessaire pour Ãªtre pÃ©dagogique]  
Step 2 : [...]  
Step 3 : [...]  
...

ğŸ’¡ **Conseils supplÃ©mentaires ou prÃ©cautions Ã  prendre** :
[Ajoute ici des recommandations pratiques, conseils dâ€™outillage, prÃ©cautions de sÃ©curitÃ©, ou bonnes pratiques.]

---

ğŸ¯ Important : Structure ta rÃ©ponse de maniÃ¨re fluide et rigoureuse. Sois synthÃ©tique mais informatif, professionnel dans le ton, et prÃ©cis dans les formulations.
""",
)


def get_guide_steps(guideid):
    url = f"https://www.ifixit.com/api/2.0/guides/{guideid}"
    response = requests.get(url)

    if response.status_code != 200:
        return {
            "error": f"Ã‰chec de rÃ©cupÃ©ration du guide {guideid}, code: {response.status_code}"
        }

    data = response.json()
    steps = []

    cpt_steps = 0

    for step in data.get("steps", []):
        cpt_steps += 1
        step_texts = [
            line["text_rendered"]
            for line in step.get("lines", [])
            if "text_rendered" in line
        ]
        steps.append({"stepno": cpt_steps, "text": step_texts})

    return steps


# Fonction qui rÃ©cupÃ¨re le contenu des documents trouvÃ©s
def format_documents(docs):
    for doc in docs:
        guide_steps = get_guide_steps(doc.metadata.get("guideid"))

        doc.page_content += "\n".join(
            f"Step {step['stepno']}:\n" + "\n".join(step["text"])
            for step in guide_steps
        )

    print(docs)

    return "\n\n".join(doc.page_content for doc in docs)


# ChaÃ®ne RAG
def create_rag_chain(retriever):
    return (
        {
            "context": retriever | format_documents,
            "question": RunnablePassthrough(),
        }
        | prompt_template
        | llm
        | StrOutputParser()
    )
